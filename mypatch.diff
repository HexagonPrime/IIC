diff --git a/.idea/.gitignore b/.idea/.gitignore
new file mode 100644
index 0000000..e7e9d11
--- /dev/null
+++ b/.idea/.gitignore
@@ -0,0 +1,2 @@
+# Default ignored files
+/workspace.xml
diff --git a/.idea/IIC.iml b/.idea/IIC.iml
new file mode 100644
index 0000000..7637020
--- /dev/null
+++ b/.idea/IIC.iml
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$" />
+    <orderEntry type="jdk" jdkName="Python 2.7 (AINcw)" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+</module>
\ No newline at end of file
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
index 0000000..105ce2d
--- /dev/null
+++ b/.idea/inspectionProfiles/profiles_settings.xml
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
index 0000000..c75d553
--- /dev/null
+++ b/.idea/misc.xml
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 2.7 (AINcw)" project-jdk-type="Python SDK" />
+</project>
\ No newline at end of file
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100644
index 0000000..2e16d43
--- /dev/null
+++ b/.idea/modules.xml
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/IIC.iml" filepath="$PROJECT_DIR$/.idea/IIC.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100644
index 0000000..94a25f7
--- /dev/null
+++ b/.idea/vcs.xml
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
diff --git a/HPC_command.txt b/HPC_command.txt
new file mode 100644
index 0000000..79d508d
--- /dev/null
+++ b/HPC_command.txt
@@ -0,0 +1,2 @@
+sbatch -p nms_teach_gpu stl10_test_1.sh
+cat /mnt/lustre/users/k1763920/39263.out
diff --git a/code/datasets/clustering/YT_BB.py b/code/datasets/clustering/YT_BB.py
new file mode 100644
index 0000000..5e25136
--- /dev/null
+++ b/code/datasets/clustering/YT_BB.py
@@ -0,0 +1,82 @@
+import os
+import torch
+import pandas as pd
+import numpy as np
+from torchvision import transforms
+from torch.utils.data.dataset import Dataset 
+from PIL import Image
+
+class YT_BB(Dataset):
+    """
+    Arguments:
+        root: CSV files path
+        transform: desired transformation
+        frame: which frame to take
+        crop: whether crop by the bounding box
+    """
+    
+    def __init__(self, root, transform, frame, crop, partition):
+	self.root = root
+        frame = frame % 19
+        if frame > 9:
+            frame = 18 - frame
+        csv_path_train = root + '/frame' + str(frame) + '_train' + '.csv'
+        csv_path_test = root + '/frame' + str(frame) + '_test' + '.csv'
+        self.transform = transform
+        self.crop = crop
+        print 'Crop: ' + str(self.crop)
+        print 'Frame: ' + str(frame)
+
+        if partition == 'train':
+            tmp_df = pd.DataFrame.from_csv(csv_path_train, header=None, index_col=False)
+        elif partition == 'test':
+            tmp_df = pd.DataFrame.from_csv(csv_path_test, header=None, index_col=False)
+        elif partition == 'train+test':
+            tmp_df_train = pd.DataFrame.from_csv(csv_path_train, header=None, index_col=False)
+            tmp_df_test = pd.DataFrame.from_csv(csv_path_test, header=None, index_col=False)
+            tmp_df = pd.concat([tmp_df_train, tmp_df_test])
+        else:
+            assert(False)
+
+        col_names = ['segment_id', 'class_id', 'path', 'timestamp', 'object_presence', 'xmin', 'xmax', 'ymin', 'ymax']
+        tmp_df.columns = col_names
+
+        self.image_arr = np.asarray(tmp_df.iloc[:, 2])
+        self.label_arr = np.asarray(tmp_df.iloc[:, 1])
+        self.xmin_arr = np.asarray(tmp_df.iloc[:, 5])
+        self.xmax_arr = np.asarray(tmp_df.iloc[:, 6])
+        self.ymin_arr = np.asarray(tmp_df.iloc[:, 7])
+        self.ymax_arr = np.asarray(tmp_df.iloc[:, 8])
+        print len(self.image_arr)
+
+    def __getitem__(self, index):
+        # Get output image.
+        img = Image.open(self.root + self.image_arr[index])
+        label = self.label_arr[index]
+	
+        if self.crop:
+	    width, height = img.size
+# 	    print width
+# 	    print height
+	    left = int(width * self.xmin_arr[index])
+	    top = int(height * self.ymin_arr[index])
+	    right = int(width * self.xmax_arr[index])
+	    bottom = int(height * self.ymax_arr[index])
+# 	    print left
+# 	    print top
+# 	    print right
+# 	    print bottom
+	    img = img.crop((left, top, right, bottom))
+	    #new_width, new_height = img.size
+	    img = transforms.Resize([32,32])(img)
+# 	    print new_width
+# 	    print new_height
+# 	    img.show()
+	
+	#img = img.convert('RGB')
+        if self.transform is not None:
+            img = self.transform(img)
+        return img, label
+
+    def __len__(self):
+        return len(self.image_arr)
diff --git a/code/datasets/clustering/YT_BB_5.py b/code/datasets/clustering/YT_BB_5.py
new file mode 100644
index 0000000..377ba81
--- /dev/null
+++ b/code/datasets/clustering/YT_BB_5.py
@@ -0,0 +1,86 @@
+import os
+import torch
+import pandas as pd
+from torchvision import transforms
+from torch.utils.data.dataset import Dataset 
+from PIL import Image
+
+class YT_BB_5(Dataset):
+    """
+    Arguments:
+        root: CSV files path
+        transform: desired transformation
+        frame: which frame to take
+        crop: whether crop by the bounding box
+    """
+    
+    def __init__(self, root, transform, frame, crop):
+	self.root = root
+        self.csv_path = root + '/yt_bb.csv'
+        self.transform = transform
+        self.crop = crop
+        print 'Crop: ' + str(self.crop)
+        print 'Frame: ' + str(frame)
+
+        # classes: 0:bird, 6:airplane, 9:car, 3:cat, 8:dog
+        classes_needed = [0, 3, 6, 8, 9]
+        included = [0, 0, 0, 0, 0]
+        self.mapping = {0:0, 3:1, 6:2, 8:3, 9:4}
+
+        tmp_df = pd.DataFrame.from_csv(self.csv_path, header=None, index_col=False)
+        col_names = ['segment_id', 'class_id', 'path', 'timestamp', 'object_presence', 'xmin', 'xmax', 'ymin', 'ymax']
+        tmp_df.columns = col_names
+	# Choose only 5 classes
+        tmp_df = tmp_df[tmp_df['class_id'].isin(classes_needed)]
+        
+        # Get list of unique video segment files
+        groups = tmp_df.groupby('segment_id')
+        self.dataset = []
+
+        for name, group in groups:
+            # Circular if this frame required exceeds frames the segment has.
+            # this_frame = frame % len(group)
+            # Choose the last frame if the required frame does not exist
+            if frame > len(group)-1:
+                frame = len(group)-1
+            this_row = group.iloc[[frame]]
+            this_class = this_row['class_id'].iat[0]
+            if included[self.mapping.get(this_class)] < 1000:
+                self.dataset.append(this_row)
+                included[self.mapping.get(this_class)] = included[self.mapping.get(this_class)] + 1
+        print 'Dataset size: ' + str(len(self.dataset))
+
+    def __getitem__(self, index):
+        this_row = self.dataset[index]
+
+        # Get output image.
+        img = Image.open(self.root + this_row['path'].iat[0])
+	
+        if self.crop:
+	    width, height = img.size
+# 	    print width
+# 	    print height
+	    left = int(width * this_row['xmin'].iat[0])
+	    top = int(height * this_row['ymin'].iat[0])
+	    right = int(width * this_row['xmax'].iat[0])
+	    bottom = int(height * this_row['ymax'].iat[0])
+# 	    print left
+# 	    print top
+# 	    print right
+# 	    print bottom
+	    img = img.crop((left, top, right, bottom))
+	    #new_width, new_height = img.size
+	    img = transforms.Resize([32,32])(img)
+# 	    print new_width
+# 	    print new_height
+# 	    img.show()
+	
+	#img = img.convert('RGB')
+        if self.transform is not None:
+            img = self.transform(img)
+        label = this_row['class_id'].iat[0]
+        label = self.mapping.get(label)
+        return img, label
+
+    def __len__(self):
+        return len(self.dataset)
diff --git a/code/scripts/cluster/YT_BB_script.py b/code/scripts/cluster/YT_BB_script.py
new file mode 100644
index 0000000..b7fd95e
--- /dev/null
+++ b/code/scripts/cluster/YT_BB_script.py
@@ -0,0 +1,496 @@
+from __future__ import print_function
+
+import argparse
+import itertools
+import os
+import pickle
+import sys
+from datetime import datetime
+
+import matplotlib
+import numpy as np
+import torch
+
+matplotlib.use('Agg')
+import matplotlib.pyplot as plt
+
+import code.archs as archs
+from code.utils.cluster.general import config_to_str, get_opt, update_lr, nice
+from code.utils.cluster.transforms import sobel_process
+from code.utils.cluster.cluster_eval import cluster_eval, get_subhead_using_loss
+#from code.utils.cluster.data import cluster_twohead_create_dataloaders
+from code.utils.cluster.YT_BB_data import cluster_twohead_create_YT_BB_dataloaders
+from code.utils.cluster.IID_losses import IID_loss
+
+"""
+  Fully unsupervised clustering ("IIC" = "IID").
+  Train and test script (coloured datasets).
+  Network has two heads, for overclustering and final clustering.
+"""
+
+# Options ----------------------------------------------------------------------
+
+parser = argparse.ArgumentParser()
+parser.add_argument("--model_ind", type=int, required=True)
+parser.add_argument("--arch", type=str, required=True)
+parser.add_argument("--opt", type=str, default="Adam")
+parser.add_argument("--mode", type=str, default="IID")
+
+parser.add_argument("--dataset", type=str, required=True)
+parser.add_argument("--dataset_root", type=str, required=True)
+
+parser.add_argument("--gt_k", type=int, required=True)
+parser.add_argument("--output_k_A", type=int, required=True)
+parser.add_argument("--output_k_B", type=int, required=True)
+
+parser.add_argument("--lamb", type=float, default=1.0)
+parser.add_argument("--lr", type=float, default=0.01)
+parser.add_argument("--lr_schedule", type=int, nargs="+", default=[])
+parser.add_argument("--lr_mult", type=float, default=0.1)
+
+parser.add_argument("--num_epochs", type=int, default=1000)
+parser.add_argument("--batch_sz", type=int, required=True)  # num pairs
+parser.add_argument("--num_dataloaders", type=int, default=3)
+parser.add_argument("--num_sub_heads", type=int, default=5)  # per head...
+
+parser.add_argument("--out_root", type=str,
+                    default="/content/drive/My Drive/out")
+parser.add_argument("--restart", dest="restart", default=False,
+                    action="store_true")
+parser.add_argument("--restart_from_best", dest="restart_from_best",
+                    default=False, action="store_true")
+parser.add_argument("--test_code", dest="test_code", default=False,
+                    action="store_true")
+
+parser.add_argument("--stl_leave_out_unlabelled", default=False,
+                    action="store_true")
+
+parser.add_argument("--save_freq", type=int, default=10)
+
+parser.add_argument("--double_eval", default=False, action="store_true")
+
+parser.add_argument("--head_A_first", default=False, action="store_true")
+parser.add_argument("--head_A_epochs", type=int, default=1)
+parser.add_argument("--head_B_epochs", type=int, default=1)
+
+parser.add_argument("--batchnorm_track", default=False, action="store_true")
+
+parser.add_argument("--select_sub_head_on_loss", default=False,
+                    action="store_true")
+
+# transforms
+parser.add_argument("--mix_train", dest="mix_train", default=False,
+                    action="store_true")
+parser.add_argument("--include_rgb", dest="include_rgb", default=False,
+                    action="store_true")
+
+parser.add_argument("--demean", dest="demean", default=False,
+                    action="store_true")
+parser.add_argument("--per_img_demean", dest="per_img_demean", default=False,
+                    action="store_true")
+parser.add_argument("--data_mean", type=float, nargs="+", default=[])
+parser.add_argument("--data_std", type=float, nargs="+", default=[])
+
+parser.add_argument("--crop_orig", dest="crop_orig", default=False,
+                    action="store_true")
+parser.add_argument("--rand_crop_sz", type=int, default=84)
+parser.add_argument("--input_sz", type=int, default=96)
+
+parser.add_argument("--fluid_warp", dest="fluid_warp", default=False,
+                    action="store_true")
+parser.add_argument("--rand_crop_szs_tf", type=int, nargs="+",
+                    default=[])  # only used if fluid warp true
+parser.add_argument("--rot_val", type=float,
+                    default=0.)  # only used if fluid warp true
+
+parser.add_argument("--cutout", default=False, action="store_true")
+parser.add_argument("--cutout_p", type=float, default=0.5)
+parser.add_argument("--cutout_max_box", type=float, default=0.5)
+
+# the very base frame
+parser.add_argument("--base_frame", type=int, default=0)
+# interval between bases
+parser.add_argument("--base_interval", type=int, default=1)
+# number of base frame to use
+parser.add_argument("--base_num", type=int, default=1)
+# interval between input pairs
+parser.add_argument("--interval", type=int, default=0)
+# whether crop the image by bounding boxes
+parser.add_argument("--crop_by_bb", dest="crop_by_bb", default=False,
+                    action="store_true")
+# whether includes the increment on sample repeats
+parser.add_argument("--frame_increment", dest="frame_increment", default=False,
+                    action="store_true")
+parser.add_argument("--train_partition", type=str, default='train')
+parser.add_argument("--test_partition", type=str, default='test')
+parser.add_argument("--assignment_partition", type=str, default='train')
+
+parser.add_argument("--random_crop", dest="random_crop", default=False,
+                    action="store_true")
+parser.add_argument("--test_on_all_frame", dest="test_on_all_frame", default=False,
+                    action="store_true")
+parser.add_argument("--rand_crop_x_prime", dest="rand_crop_x_prime", default=False,
+                    action="store_true")
+parser.add_argument("--remove_g", dest="remove_g", default=False,
+                    action="store_true")
+parser.add_argument("--center_crop_x", dest="center_crop_x", default=False,
+                    action="store_true")
+parser.add_argument("--center_crop_x_prime", dest="center_crop_x_prime", default=False,
+                    action="store_true")
+config = parser.parse_args()
+
+# Setup ------------------------------------------------------------------------
+
+config.twohead = True
+
+if not config.include_rgb:
+    config.in_channels = 2
+else:
+    config.in_channels = 5
+
+config.out_dir = os.path.join(config.out_root, str(config.model_ind))
+assert (config.batch_sz % config.num_dataloaders == 0)
+config.dataloader_batch_sz = config.batch_sz / config.num_dataloaders
+
+assert (config.mode == "IID")
+assert ("TwoHead" in config.arch)
+assert (config.output_k_B == config.gt_k)
+config.output_k = config.output_k_B  # for eval code
+assert (config.output_k_A >= config.gt_k)
+
+config.eval_mode = "hung"
+
+if not os.path.exists(config.out_dir):
+    os.makedirs(config.out_dir)
+
+if config.restart:
+    config_name = "config.pickle"
+    net_name = "latest_net.pytorch"
+    opt_name = "latest_optimiser.pytorch"
+
+    if config.restart_from_best:
+        config_name = "best_config.pickle"
+        net_name = "best_net.pytorch"
+        opt_name = "best_optimiser.pytorch"
+
+    given_config = config
+    reloaded_config_path = os.path.join(given_config.out_dir, config_name)
+    print("Loading restarting config from: %s" % reloaded_config_path)
+    with open(reloaded_config_path, "rb") as config_f:
+        config = pickle.load(config_f)
+    assert (config.model_ind == given_config.model_ind)
+    config.restart = True
+    config.restart_from_best = given_config.restart_from_best
+
+    # copy over new num_epochs and lr schedule
+    config.num_epochs = given_config.num_epochs
+    config.lr_schedule = given_config.lr_schedule
+
+    if not hasattr(config, "cutout"):
+        config.cutout = False
+        config.cutout_p = 0.5
+        config.cutout_max_box = 0.5
+
+    if not hasattr(config, "batchnorm_track"):
+        config.batchnorm_track = True  # before we added in false option
+
+else:
+    print("Config: %s" % config_to_str(config))
+
+# Model ------------------------------------------------------------------------
+
+dataloaders_head_A, dataloaders_head_B, mapping_assignment_dataloader, \
+mapping_test_dataloader = cluster_twohead_create_YT_BB_dataloaders(config)
+
+net = archs.__dict__[config.arch](config)
+if config.restart:
+    model_path = os.path.join(config.out_dir, net_name)
+    net.load_state_dict(
+        torch.load(model_path, map_location=lambda storage, loc: storage))
+
+net.cuda()
+net = torch.nn.DataParallel(net)
+net.train()
+
+optimiser = get_opt(config.opt)(net.module.parameters(), lr=config.lr)
+if config.restart:
+    opt_path = os.path.join(config.out_dir, opt_name)
+    optimiser.load_state_dict(torch.load(opt_path))
+
+heads = ["B", "A"]
+if config.head_A_first:
+    heads = ["A", "B"]
+
+head_epochs = {}
+head_epochs["A"] = config.head_A_epochs
+head_epochs["B"] = config.head_B_epochs
+
+# Results ----------------------------------------------------------------------
+
+if config.restart:
+    if not config.restart_from_best:
+        next_epoch = config.last_epoch + 1  # corresponds to last saved model
+    else:
+        # sanity check
+        next_epoch = np.argmax(np.array(config.epoch_acc)) + 1
+        assert (next_epoch == config.last_epoch + 1)
+    print("starting from epoch %d" % next_epoch)
+
+    config.epoch_acc = config.epoch_acc[:next_epoch]  # in case we overshot
+    config.epoch_avg_subhead_acc = config.epoch_avg_subhead_acc[:next_epoch]
+    config.epoch_stats = config.epoch_stats[:next_epoch]
+
+    if config.double_eval:
+        config.double_eval_acc = config.double_eval_acc[:next_epoch]
+        config.double_eval_avg_subhead_acc = config.double_eval_avg_subhead_acc[
+                                             :next_epoch]
+        config.double_eval_stats = config.double_eval_stats[:next_epoch]
+
+    config.epoch_loss_head_A = config.epoch_loss_head_A[:(next_epoch - 1)]
+    config.epoch_loss_no_lamb_head_A = config.epoch_loss_no_lamb_head_A[
+                                       :(next_epoch - 1)]
+    config.epoch_loss_head_B = config.epoch_loss_head_B[:(next_epoch - 1)]
+    config.epoch_loss_no_lamb_head_B = config.epoch_loss_no_lamb_head_B[
+                                       :(next_epoch - 1)]
+else:
+    config.epoch_acc = []
+    config.epoch_avg_subhead_acc = []
+    config.epoch_stats = []
+    config.epoch_train_acc = []
+    config.epoch_test_acc_2 = []
+
+    if config.double_eval:
+        config.double_eval_acc = []
+        config.double_eval_avg_subhead_acc = []
+        config.double_eval_stats = []
+        config.epoch_train_acc = []
+        config.epoch_test_acc_2 = []
+
+    config.epoch_loss_head_A = []
+    config.epoch_loss_no_lamb_head_A = []
+
+    config.epoch_loss_head_B = []
+    config.epoch_loss_no_lamb_head_B = []
+
+    sub_head = None
+    if config.select_sub_head_on_loss:
+        sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
+                                          sobel=True, lamb=config.lamb)
+    _ = cluster_eval(config, net,
+                     mapping_assignment_dataloader=mapping_assignment_dataloader,
+                     mapping_test_dataloader=mapping_test_dataloader,
+                     sobel=True,
+                     use_sub_head=sub_head)
+
+    print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
+    if config.double_eval:
+        print("double eval: \n %s" % (nice(config.double_eval_stats[-1])))
+    sys.stdout.flush()
+    next_epoch = 1
+
+fig, axarr = plt.subplots(6 + 2 * int(config.double_eval), sharex=False,
+                          figsize=(20, 20))
+
+# Train ------------------------------------------------------------------------
+
+for e_i in xrange(next_epoch, config.num_epochs):
+    print("Starting e_i: %d" % (e_i))
+
+    if e_i in config.lr_schedule:
+        optimiser = update_lr(optimiser, lr_mult=config.lr_mult)
+
+    for head_i in range(2):
+        head = heads[head_i]
+        if head == "A":
+            dataloaders = dataloaders_head_A
+            epoch_loss = config.epoch_loss_head_A
+            epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_A
+        elif head == "B":
+            dataloaders = dataloaders_head_B
+            epoch_loss = config.epoch_loss_head_B
+            epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_B
+
+        avg_loss = 0.  # over heads and head_epochs (and sub_heads)
+        avg_loss_no_lamb = 0.
+        avg_loss_count = 0
+
+        for head_i_epoch in range(head_epochs[head]):
+            sys.stdout.flush()
+
+            iterators = (d for d in dataloaders)
+
+            b_i = 0
+            for tup in itertools.izip(*iterators):
+                net.module.zero_grad()
+
+                # one less because this is before sobel
+                all_imgs = torch.zeros(config.batch_sz, config.in_channels - 1,
+                                       config.input_sz,
+                                       config.input_sz).cuda()
+                all_imgs_tf = torch.zeros(config.batch_sz, config.in_channels - 1,
+                                          config.input_sz,
+                                          config.input_sz).cuda()
+
+                imgs_curr = tup[0][0]  # always the first
+                curr_batch_sz = imgs_curr.size(0)
+                for d_i in xrange(config.num_dataloaders):
+                    imgs_tf_curr = tup[1 + d_i][0]  # from 2nd to last
+                    assert (curr_batch_sz == imgs_tf_curr.size(0))
+
+                    actual_batch_start = d_i * curr_batch_sz
+                    actual_batch_end = actual_batch_start + curr_batch_sz
+                    all_imgs[actual_batch_start:actual_batch_end, :, :, :] = \
+                        imgs_curr.cuda()
+                    all_imgs_tf[actual_batch_start:actual_batch_end, :, :, :] = \
+                        imgs_tf_curr.cuda()
+
+                if not (curr_batch_sz == config.dataloader_batch_sz):
+                    print("last batch sz %d" % curr_batch_sz)
+
+                curr_total_batch_sz = curr_batch_sz * config.num_dataloaders
+                all_imgs = all_imgs[:curr_total_batch_sz, :, :, :]
+                all_imgs_tf = all_imgs_tf[:curr_total_batch_sz, :, :, :]
+
+                all_imgs = sobel_process(all_imgs, config.include_rgb)
+                all_imgs_tf = sobel_process(all_imgs_tf, config.include_rgb)
+
+                x_outs = net(all_imgs, head=head)
+                x_tf_outs = net(all_imgs_tf, head=head)
+
+                avg_loss_batch = None  # avg over the sub_heads
+                avg_loss_no_lamb_batch = None
+                for i in xrange(config.num_sub_heads):
+                    loss, loss_no_lamb = IID_loss(x_outs[i], x_tf_outs[i],
+                                                  lamb=config.lamb)
+                    if avg_loss_batch is None:
+                        avg_loss_batch = loss
+                        avg_loss_no_lamb_batch = loss_no_lamb
+                    else:
+                        avg_loss_batch += loss
+                        avg_loss_no_lamb_batch += loss_no_lamb
+
+                avg_loss_batch /= config.num_sub_heads
+                avg_loss_no_lamb_batch /= config.num_sub_heads
+
+                if ((b_i % 100) == 0) or (e_i == next_epoch and b_i < 10):
+                    print("Model ind %d epoch %d head %s head_i_epoch %d batch %d: avg "
+                          "loss %f avg loss no lamb %f time %s" % \
+                          (config.model_ind, e_i, head, head_i_epoch, b_i,
+                           avg_loss_batch.item(), avg_loss_no_lamb_batch.item(),
+                           datetime.now()))
+                    sys.stdout.flush()
+
+                if not np.isfinite(avg_loss_batch.item()):
+                    print("Loss is not finite... %s:" % str(avg_loss_batch))
+                    exit(1)
+
+                avg_loss += avg_loss_batch.item()
+                avg_loss_no_lamb += avg_loss_no_lamb_batch.item()
+                avg_loss_count += 1
+
+                avg_loss_batch.backward()
+                optimiser.step()
+
+                b_i += 1
+                if b_i == 2 and config.test_code:
+                    break
+
+        avg_loss = float(avg_loss / avg_loss_count)
+        avg_loss_no_lamb = float(avg_loss_no_lamb / avg_loss_count)
+
+        epoch_loss.append(avg_loss)
+        epoch_loss_no_lamb.append(avg_loss_no_lamb)
+
+    # Eval -----------------------------------------------------------------------
+
+    # Can also pick the subhead using the evaluation process (to do this,
+    #  set use_sub_head=None)
+    sub_head = None
+    if config.select_sub_head_on_loss:
+        sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
+                                          sobel=True, lamb=config.lamb)
+    is_best = cluster_eval(config, net,
+                           mapping_assignment_dataloader=mapping_assignment_dataloader,
+                           mapping_test_dataloader=mapping_test_dataloader,
+                           sobel=True,
+                           use_sub_head=sub_head)
+
+    print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
+    if config.double_eval:
+        print("     double eval: \n %s" % (nice(config.double_eval_stats[-1])))
+    sys.stdout.flush()
+
+    axarr[0].clear()
+    axarr[0].plot(config.epoch_acc)
+    axarr[0].set_title("acc (best), top: %f" % max(config.epoch_acc))
+
+    axarr[1].clear()
+    axarr[1].plot(config.epoch_avg_subhead_acc)
+    axarr[1].set_title("acc (avg), top: %f" % max(config.epoch_avg_subhead_acc))
+
+    axarr[2].clear()
+    axarr[2].plot(config.epoch_loss_head_A)
+    axarr[2].set_title("Loss head A")
+
+    axarr[3].clear()
+    axarr[3].plot(config.epoch_loss_no_lamb_head_A)
+    axarr[3].set_title("Loss no lamb head A")
+
+    axarr[4].clear()
+    axarr[4].plot(config.epoch_loss_head_B)
+    axarr[4].set_title("Loss head B")
+
+    axarr[5].clear()
+    axarr[5].plot(config.epoch_loss_no_lamb_head_B)
+    axarr[5].set_title("Loss no lamb head B")
+
+    if config.double_eval:
+        axarr[6].clear()
+        axarr[6].plot(config.double_eval_acc)
+        axarr[6].set_title("double eval acc (best), top: %f" %
+                           max(config.double_eval_acc))
+
+        axarr[7].clear()
+        axarr[7].plot(config.double_eval_avg_subhead_acc)
+        axarr[7].set_title("double eval acc (avg), top: %f" %
+                           max(config.double_eval_avg_subhead_acc))
+
+    fig.tight_layout()
+    fig.canvas.draw_idle()
+    fig.savefig(os.path.join(config.out_dir, "plots.png"))
+
+    if is_best or (e_i % config.save_freq == 0):
+        net.module.cpu()
+
+        if e_i % config.save_freq == 0:
+            torch.save(net.module.state_dict(),
+                       os.path.join(config.out_dir, "latest_net.pytorch"))
+            torch.save(optimiser.state_dict(),
+                       os.path.join(config.out_dir, "latest_optimiser.pytorch"))
+            config.last_epoch = e_i  # for last saved version
+
+        if is_best:
+            torch.save(net.module.state_dict(),
+                       os.path.join(config.out_dir, "best_net.pytorch"))
+            torch.save(optimiser.state_dict(),
+                       os.path.join(config.out_dir, "best_optimiser.pytorch"))
+            with open(os.path.join(config.out_dir, "best_config.pickle"),
+                      'wb') as outfile:
+                pickle.dump(config, outfile)
+
+            with open(os.path.join(config.out_dir, "best_config.txt"),
+                      "w") as text_file:
+                text_file.write("%s" % config)
+
+        net.module.cuda()
+
+    with open(os.path.join(config.out_dir, "config.pickle"),
+              'wb') as outfile:
+        pickle.dump(config, outfile)
+
+    with open(os.path.join(config.out_dir, "config.txt"),
+              "w") as text_file:
+        text_file.write("%s" % config)
+
+    if config.test_code:
+        exit(0)
diff --git a/code/scripts/cluster/cluster_sobel_twohead.py b/code/scripts/cluster/cluster_sobel_twohead.py
index 68212ea..da1dc7c 100644
--- a/code/scripts/cluster/cluster_sobel_twohead.py
+++ b/code/scripts/cluster/cluster_sobel_twohead.py
@@ -53,7 +53,7 @@ parser.add_argument("--num_dataloaders", type=int, default=3)
 parser.add_argument("--num_sub_heads", type=int, default=5)  # per head...
 
 parser.add_argument("--out_root", type=str,
-                    default="/scratch/shared/slow/xuji/iid_private")
+                    default="/users/k1763920/IIC/out")
 parser.add_argument("--restart", dest="restart", default=False,
                     action="store_true")
 parser.add_argument("--restart_from_best", dest="restart_from_best",
@@ -113,9 +113,9 @@ config = parser.parse_args()
 config.twohead = True
 
 if not config.include_rgb:
-  config.in_channels = 2
+    config.in_channels = 2
 else:
-  config.in_channels = 5
+    config.in_channels = 5
 
 config.out_dir = os.path.join(config.out_root, str(config.model_ind))
 assert (config.batch_sz % config.num_dataloaders == 0)
@@ -130,41 +130,41 @@ assert (config.output_k_A >= config.gt_k)
 config.eval_mode = "hung"
 
 if not os.path.exists(config.out_dir):
-  os.makedirs(config.out_dir)
+    os.makedirs(config.out_dir)
 
 if config.restart:
-  config_name = "config.pickle"
-  net_name = "latest_net.pytorch"
-  opt_name = "latest_optimiser.pytorch"
-
-  if config.restart_from_best:
-    config_name = "best_config.pickle"
-    net_name = "best_net.pytorch"
-    opt_name = "best_optimiser.pytorch"
-
-  given_config = config
-  reloaded_config_path = os.path.join(given_config.out_dir, config_name)
-  print("Loading restarting config from: %s" % reloaded_config_path)
-  with open(reloaded_config_path, "rb") as config_f:
-    config = pickle.load(config_f)
-  assert (config.model_ind == given_config.model_ind)
-  config.restart = True
-  config.restart_from_best = given_config.restart_from_best
-
-  # copy over new num_epochs and lr schedule
-  config.num_epochs = given_config.num_epochs
-  config.lr_schedule = given_config.lr_schedule
-
-  if not hasattr(config, "cutout"):
-    config.cutout = False
-    config.cutout_p = 0.5
-    config.cutout_max_box = 0.5
-
-  if not hasattr(config, "batchnorm_track"):
-    config.batchnorm_track = True  # before we added in false option
+    config_name = "config.pickle"
+    net_name = "latest_net.pytorch"
+    opt_name = "latest_optimiser.pytorch"
+
+    if config.restart_from_best:
+        config_name = "best_config.pickle"
+        net_name = "best_net.pytorch"
+        opt_name = "best_optimiser.pytorch"
+
+    given_config = config
+    reloaded_config_path = os.path.join(given_config.out_dir, config_name)
+    print("Loading restarting config from: %s" % reloaded_config_path)
+    with open(reloaded_config_path, "rb") as config_f:
+        config = pickle.load(config_f)
+    assert (config.model_ind == given_config.model_ind)
+    config.restart = True
+    config.restart_from_best = given_config.restart_from_best
+
+    # copy over new num_epochs and lr schedule
+    config.num_epochs = given_config.num_epochs
+    config.lr_schedule = given_config.lr_schedule
+
+    if not hasattr(config, "cutout"):
+        config.cutout = False
+        config.cutout_p = 0.5
+        config.cutout_max_box = 0.5
+
+    if not hasattr(config, "batchnorm_track"):
+        config.batchnorm_track = True  # before we added in false option
 
 else:
-  print("Config: %s" % config_to_str(config))
+    print("Config: %s" % config_to_str(config))
 
 # Model ------------------------------------------------------------------------
 
@@ -173,9 +173,9 @@ mapping_test_dataloader = cluster_twohead_create_dataloaders(config)
 
 net = archs.__dict__[config.arch](config)
 if config.restart:
-  model_path = os.path.join(config.out_dir, net_name)
-  net.load_state_dict(
-    torch.load(model_path, map_location=lambda storage, loc: storage))
+    model_path = os.path.join(config.out_dir, net_name)
+    net.load_state_dict(
+        torch.load(model_path, map_location=lambda storage, loc: storage))
 
 net.cuda()
 net = torch.nn.DataParallel(net)
@@ -183,12 +183,12 @@ net.train()
 
 optimiser = get_opt(config.opt)(net.module.parameters(), lr=config.lr)
 if config.restart:
-  opt_path = os.path.join(config.out_dir, opt_name)
-  optimiser.load_state_dict(torch.load(opt_path))
+    opt_path = os.path.join(config.out_dir, opt_name)
+    optimiser.load_state_dict(torch.load(opt_path))
 
 heads = ["B", "A"]
 if config.head_A_first:
-  heads = ["A", "B"]
+    heads = ["A", "B"]
 
 head_epochs = {}
 head_epochs["A"] = config.head_A_epochs
@@ -197,61 +197,61 @@ head_epochs["B"] = config.head_B_epochs
 # Results ----------------------------------------------------------------------
 
 if config.restart:
-  if not config.restart_from_best:
-    next_epoch = config.last_epoch + 1  # corresponds to last saved model
-  else:
-    # sanity check
-    next_epoch = np.argmax(np.array(config.epoch_acc)) + 1
-    assert (next_epoch == config.last_epoch + 1)
-  print("starting from epoch %d" % next_epoch)
-
-  config.epoch_acc = config.epoch_acc[:next_epoch]  # in case we overshot
-  config.epoch_avg_subhead_acc = config.epoch_avg_subhead_acc[:next_epoch]
-  config.epoch_stats = config.epoch_stats[:next_epoch]
-
-  if config.double_eval:
-    config.double_eval_acc = config.double_eval_acc[:next_epoch]
-    config.double_eval_avg_subhead_acc = config.double_eval_avg_subhead_acc[
-                                         :next_epoch]
-    config.double_eval_stats = config.double_eval_stats[:next_epoch]
-
-  config.epoch_loss_head_A = config.epoch_loss_head_A[:(next_epoch - 1)]
-  config.epoch_loss_no_lamb_head_A = config.epoch_loss_no_lamb_head_A[
-                                     :(next_epoch - 1)]
-  config.epoch_loss_head_B = config.epoch_loss_head_B[:(next_epoch - 1)]
-  config.epoch_loss_no_lamb_head_B = config.epoch_loss_no_lamb_head_B[
-                                     :(next_epoch - 1)]
+    if not config.restart_from_best:
+        next_epoch = config.last_epoch + 1  # corresponds to last saved model
+    else:
+        # sanity check
+        next_epoch = np.argmax(np.array(config.epoch_acc)) + 1
+        assert (next_epoch == config.last_epoch + 1)
+    print("starting from epoch %d" % next_epoch)
+
+    config.epoch_acc = config.epoch_acc[:next_epoch]  # in case we overshot
+    config.epoch_avg_subhead_acc = config.epoch_avg_subhead_acc[:next_epoch]
+    config.epoch_stats = config.epoch_stats[:next_epoch]
+
+    if config.double_eval:
+        config.double_eval_acc = config.double_eval_acc[:next_epoch]
+        config.double_eval_avg_subhead_acc = config.double_eval_avg_subhead_acc[
+                                             :next_epoch]
+        config.double_eval_stats = config.double_eval_stats[:next_epoch]
+
+    config.epoch_loss_head_A = config.epoch_loss_head_A[:(next_epoch - 1)]
+    config.epoch_loss_no_lamb_head_A = config.epoch_loss_no_lamb_head_A[
+                                       :(next_epoch - 1)]
+    config.epoch_loss_head_B = config.epoch_loss_head_B[:(next_epoch - 1)]
+    config.epoch_loss_no_lamb_head_B = config.epoch_loss_no_lamb_head_B[
+                                       :(next_epoch - 1)]
 else:
-  config.epoch_acc = []
-  config.epoch_avg_subhead_acc = []
-  config.epoch_stats = []
-
-  if config.double_eval:
-    config.double_eval_acc = []
-    config.double_eval_avg_subhead_acc = []
-    config.double_eval_stats = []
-
-  config.epoch_loss_head_A = []
-  config.epoch_loss_no_lamb_head_A = []
-
-  config.epoch_loss_head_B = []
-  config.epoch_loss_no_lamb_head_B = []
-
-  sub_head = None
-  if config.select_sub_head_on_loss:
-    sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
-                                      sobel=True, lamb=config.lamb)
-  _ = cluster_eval(config, net,
-                   mapping_assignment_dataloader=mapping_assignment_dataloader,
-                   mapping_test_dataloader=mapping_test_dataloader,
-                   sobel=True,
-                   use_sub_head=sub_head)
-
-  print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
-  if config.double_eval:
-    print("double eval: \n %s" % (nice(config.double_eval_stats[-1])))
-  sys.stdout.flush()
-  next_epoch = 1
+    config.epoch_acc = []
+    config.epoch_avg_subhead_acc = []
+    config.epoch_stats = []
+
+    if config.double_eval:
+        config.double_eval_acc = []
+        config.double_eval_avg_subhead_acc = []
+        config.double_eval_stats = []
+
+    config.epoch_loss_head_A = []
+    config.epoch_loss_no_lamb_head_A = []
+
+    config.epoch_loss_head_B = []
+    config.epoch_loss_no_lamb_head_B = []
+
+    sub_head = None
+    if config.select_sub_head_on_loss:
+        sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
+                                          sobel=True, lamb=config.lamb)
+    _ = cluster_eval(config, net,
+                     mapping_assignment_dataloader=mapping_assignment_dataloader,
+                     mapping_test_dataloader=mapping_test_dataloader,
+                     sobel=True,
+                     use_sub_head=sub_head)
+
+    print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
+    if config.double_eval:
+        print("double eval: \n %s" % (nice(config.double_eval_stats[-1])))
+    sys.stdout.flush()
+    next_epoch = 1
 
 fig, axarr = plt.subplots(6 + 2 * int(config.double_eval), sharex=False,
                           figsize=(20, 20))
@@ -259,203 +259,203 @@ fig, axarr = plt.subplots(6 + 2 * int(config.double_eval), sharex=False,
 # Train ------------------------------------------------------------------------
 
 for e_i in xrange(next_epoch, config.num_epochs):
-  print("Starting e_i: %d" % (e_i))
-
-  if e_i in config.lr_schedule:
-    optimiser = update_lr(optimiser, lr_mult=config.lr_mult)
-
-  for head_i in range(2):
-    head = heads[head_i]
-    if head == "A":
-      dataloaders = dataloaders_head_A
-      epoch_loss = config.epoch_loss_head_A
-      epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_A
-    elif head == "B":
-      dataloaders = dataloaders_head_B
-      epoch_loss = config.epoch_loss_head_B
-      epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_B
-
-    avg_loss = 0.  # over heads and head_epochs (and sub_heads)
-    avg_loss_no_lamb = 0.
-    avg_loss_count = 0
-
-    for head_i_epoch in range(head_epochs[head]):
-      sys.stdout.flush()
-
-      iterators = (d for d in dataloaders)
-
-      b_i = 0
-      for tup in itertools.izip(*iterators):
-        net.module.zero_grad()
-
-        # one less because this is before sobel
-        all_imgs = torch.zeros(config.batch_sz, config.in_channels - 1,
-                               config.input_sz,
-                               config.input_sz).cuda()
-        all_imgs_tf = torch.zeros(config.batch_sz, config.in_channels - 1,
-                                  config.input_sz,
-                                  config.input_sz).cuda()
-
-        imgs_curr = tup[0][0]  # always the first
-        curr_batch_sz = imgs_curr.size(0)
-        for d_i in xrange(config.num_dataloaders):
-          imgs_tf_curr = tup[1 + d_i][0]  # from 2nd to last
-          assert (curr_batch_sz == imgs_tf_curr.size(0))
-
-          actual_batch_start = d_i * curr_batch_sz
-          actual_batch_end = actual_batch_start + curr_batch_sz
-          all_imgs[actual_batch_start:actual_batch_end, :, :, :] = \
-            imgs_curr.cuda()
-          all_imgs_tf[actual_batch_start:actual_batch_end, :, :, :] = \
-            imgs_tf_curr.cuda()
-
-        if not (curr_batch_sz == config.dataloader_batch_sz):
-          print("last batch sz %d" % curr_batch_sz)
-
-        curr_total_batch_sz = curr_batch_sz * config.num_dataloaders
-        all_imgs = all_imgs[:curr_total_batch_sz, :, :, :]
-        all_imgs_tf = all_imgs_tf[:curr_total_batch_sz, :, :, :]
-
-        all_imgs = sobel_process(all_imgs, config.include_rgb)
-        all_imgs_tf = sobel_process(all_imgs_tf, config.include_rgb)
-
-        x_outs = net(all_imgs, head=head)
-        x_tf_outs = net(all_imgs_tf, head=head)
-
-        avg_loss_batch = None  # avg over the sub_heads
-        avg_loss_no_lamb_batch = None
-        for i in xrange(config.num_sub_heads):
-          loss, loss_no_lamb = IID_loss(x_outs[i], x_tf_outs[i],
-                                        lamb=config.lamb)
-          if avg_loss_batch is None:
-            avg_loss_batch = loss
-            avg_loss_no_lamb_batch = loss_no_lamb
-          else:
-            avg_loss_batch += loss
-            avg_loss_no_lamb_batch += loss_no_lamb
-
-        avg_loss_batch /= config.num_sub_heads
-        avg_loss_no_lamb_batch /= config.num_sub_heads
-
-        if ((b_i % 100) == 0) or (e_i == next_epoch and b_i < 10):
-          print("Model ind %d epoch %d head %s head_i_epoch %d batch %d: avg "
-                "loss %f avg loss no lamb %f time %s" % \
-                (config.model_ind, e_i, head, head_i_epoch, b_i,
-                 avg_loss_batch.item(), avg_loss_no_lamb_batch.item(),
-                 datetime.now()))
-          sys.stdout.flush()
-
-        if not np.isfinite(avg_loss_batch.item()):
-          print("Loss is not finite... %s:" % str(avg_loss_batch))
-          exit(1)
-
-        avg_loss += avg_loss_batch.item()
-        avg_loss_no_lamb += avg_loss_no_lamb_batch.item()
-        avg_loss_count += 1
-
-        avg_loss_batch.backward()
-        optimiser.step()
-
-        b_i += 1
-        if b_i == 2 and config.test_code:
-          break
-
-    avg_loss = float(avg_loss / avg_loss_count)
-    avg_loss_no_lamb = float(avg_loss_no_lamb / avg_loss_count)
-
-    epoch_loss.append(avg_loss)
-    epoch_loss_no_lamb.append(avg_loss_no_lamb)
-
-  # Eval -----------------------------------------------------------------------
-
-  # Can also pick the subhead using the evaluation process (to do this,
-  #  set use_sub_head=None)
-  sub_head = None
-  if config.select_sub_head_on_loss:
-    sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
-                                      sobel=True, lamb=config.lamb)
-  is_best = cluster_eval(config, net,
-                         mapping_assignment_dataloader=mapping_assignment_dataloader,
-                         mapping_test_dataloader=mapping_test_dataloader,
-                         sobel=True,
-                         use_sub_head=sub_head)
-
-  print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
-  if config.double_eval:
-    print("     double eval: \n %s" % (nice(config.double_eval_stats[-1])))
-  sys.stdout.flush()
-
-  axarr[0].clear()
-  axarr[0].plot(config.epoch_acc)
-  axarr[0].set_title("acc (best), top: %f" % max(config.epoch_acc))
-
-  axarr[1].clear()
-  axarr[1].plot(config.epoch_avg_subhead_acc)
-  axarr[1].set_title("acc (avg), top: %f" % max(config.epoch_avg_subhead_acc))
-
-  axarr[2].clear()
-  axarr[2].plot(config.epoch_loss_head_A)
-  axarr[2].set_title("Loss head A")
-
-  axarr[3].clear()
-  axarr[3].plot(config.epoch_loss_no_lamb_head_A)
-  axarr[3].set_title("Loss no lamb head A")
-
-  axarr[4].clear()
-  axarr[4].plot(config.epoch_loss_head_B)
-  axarr[4].set_title("Loss head B")
-
-  axarr[5].clear()
-  axarr[5].plot(config.epoch_loss_no_lamb_head_B)
-  axarr[5].set_title("Loss no lamb head B")
-
-  if config.double_eval:
-    axarr[6].clear()
-    axarr[6].plot(config.double_eval_acc)
-    axarr[6].set_title("double eval acc (best), top: %f" %
-                       max(config.double_eval_acc))
-
-    axarr[7].clear()
-    axarr[7].plot(config.double_eval_avg_subhead_acc)
-    axarr[7].set_title("double eval acc (avg), top: %f" %
-                       max(config.double_eval_avg_subhead_acc))
-
-  fig.tight_layout()
-  fig.canvas.draw_idle()
-  fig.savefig(os.path.join(config.out_dir, "plots.png"))
-
-  if is_best or (e_i % config.save_freq == 0):
-    net.module.cpu()
-
-    if e_i % config.save_freq == 0:
-      torch.save(net.module.state_dict(),
-                 os.path.join(config.out_dir, "latest_net.pytorch"))
-      torch.save(optimiser.state_dict(),
-                 os.path.join(config.out_dir, "latest_optimiser.pytorch"))
-      config.last_epoch = e_i  # for last saved version
-
-    if is_best:
-      torch.save(net.module.state_dict(),
-                 os.path.join(config.out_dir, "best_net.pytorch"))
-      torch.save(optimiser.state_dict(),
-                 os.path.join(config.out_dir, "best_optimiser.pytorch"))
-      with open(os.path.join(config.out_dir, "best_config.pickle"),
-                'wb') as outfile:
+    print("Starting e_i: %d" % (e_i))
+
+    if e_i in config.lr_schedule:
+        optimiser = update_lr(optimiser, lr_mult=config.lr_mult)
+
+    for head_i in range(2):
+        head = heads[head_i]
+        if head == "A":
+            dataloaders = dataloaders_head_A
+            epoch_loss = config.epoch_loss_head_A
+            epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_A
+        elif head == "B":
+            dataloaders = dataloaders_head_B
+            epoch_loss = config.epoch_loss_head_B
+            epoch_loss_no_lamb = config.epoch_loss_no_lamb_head_B
+
+        avg_loss = 0.  # over heads and head_epochs (and sub_heads)
+        avg_loss_no_lamb = 0.
+        avg_loss_count = 0
+
+        for head_i_epoch in range(head_epochs[head]):
+            sys.stdout.flush()
+
+            iterators = (d for d in dataloaders)
+
+            b_i = 0
+            for tup in itertools.izip(*iterators):
+                net.module.zero_grad()
+
+                # one less because this is before sobel
+                all_imgs = torch.zeros(config.batch_sz, config.in_channels - 1,
+                                       config.input_sz,
+                                       config.input_sz).cuda()
+                all_imgs_tf = torch.zeros(config.batch_sz, config.in_channels - 1,
+                                          config.input_sz,
+                                          config.input_sz).cuda()
+
+                imgs_curr = tup[0][0]  # always the first
+                curr_batch_sz = imgs_curr.size(0)
+                for d_i in xrange(config.num_dataloaders):
+                    imgs_tf_curr = tup[1 + d_i][0]  # from 2nd to last
+                    assert (curr_batch_sz == imgs_tf_curr.size(0))
+
+                    actual_batch_start = d_i * curr_batch_sz
+                    actual_batch_end = actual_batch_start + curr_batch_sz
+                    all_imgs[actual_batch_start:actual_batch_end, :, :, :] = \
+                        imgs_curr.cuda()
+                    all_imgs_tf[actual_batch_start:actual_batch_end, :, :, :] = \
+                        imgs_tf_curr.cuda()
+
+                if not (curr_batch_sz == config.dataloader_batch_sz):
+                    print("last batch sz %d" % curr_batch_sz)
+
+                curr_total_batch_sz = curr_batch_sz * config.num_dataloaders
+                all_imgs = all_imgs[:curr_total_batch_sz, :, :, :]
+                all_imgs_tf = all_imgs_tf[:curr_total_batch_sz, :, :, :]
+
+                all_imgs = sobel_process(all_imgs, config.include_rgb)
+                all_imgs_tf = sobel_process(all_imgs_tf, config.include_rgb)
+
+                x_outs = net(all_imgs, head=head)
+                x_tf_outs = net(all_imgs_tf, head=head)
+
+                avg_loss_batch = None  # avg over the sub_heads
+                avg_loss_no_lamb_batch = None
+                for i in xrange(config.num_sub_heads):
+                    loss, loss_no_lamb = IID_loss(x_outs[i], x_tf_outs[i],
+                                                  lamb=config.lamb)
+                    if avg_loss_batch is None:
+                        avg_loss_batch = loss
+                        avg_loss_no_lamb_batch = loss_no_lamb
+                    else:
+                        avg_loss_batch += loss
+                        avg_loss_no_lamb_batch += loss_no_lamb
+
+                avg_loss_batch /= config.num_sub_heads
+                avg_loss_no_lamb_batch /= config.num_sub_heads
+
+                if ((b_i % 100) == 0) or (e_i == next_epoch and b_i < 10):
+                    print("Model ind %d epoch %d head %s head_i_epoch %d batch %d: avg "
+                          "loss %f avg loss no lamb %f time %s" % \
+                          (config.model_ind, e_i, head, head_i_epoch, b_i,
+                           avg_loss_batch.item(), avg_loss_no_lamb_batch.item(),
+                           datetime.now()))
+                    sys.stdout.flush()
+
+                if not np.isfinite(avg_loss_batch.item()):
+                    print("Loss is not finite... %s:" % str(avg_loss_batch))
+                    exit(1)
+
+                avg_loss += avg_loss_batch.item()
+                avg_loss_no_lamb += avg_loss_no_lamb_batch.item()
+                avg_loss_count += 1
+
+                avg_loss_batch.backward()
+                optimiser.step()
+
+                b_i += 1
+                if b_i == 2 and config.test_code:
+                    break
+
+        avg_loss = float(avg_loss / avg_loss_count)
+        avg_loss_no_lamb = float(avg_loss_no_lamb / avg_loss_count)
+
+        epoch_loss.append(avg_loss)
+        epoch_loss_no_lamb.append(avg_loss_no_lamb)
+
+    # Eval -----------------------------------------------------------------------
+
+    # Can also pick the subhead using the evaluation process (to do this,
+    #  set use_sub_head=None)
+    sub_head = None
+    if config.select_sub_head_on_loss:
+        sub_head = get_subhead_using_loss(config, dataloaders_head_B, net,
+                                          sobel=True, lamb=config.lamb)
+    is_best = cluster_eval(config, net,
+                           mapping_assignment_dataloader=mapping_assignment_dataloader,
+                           mapping_test_dataloader=mapping_test_dataloader,
+                           sobel=True,
+                           use_sub_head=sub_head)
+
+    print("Pre: time %s: \n %s" % (datetime.now(), nice(config.epoch_stats[-1])))
+    if config.double_eval:
+        print("     double eval: \n %s" % (nice(config.double_eval_stats[-1])))
+    sys.stdout.flush()
+
+    axarr[0].clear()
+    axarr[0].plot(config.epoch_acc)
+    axarr[0].set_title("acc (best), top: %f" % max(config.epoch_acc))
+
+    axarr[1].clear()
+    axarr[1].plot(config.epoch_avg_subhead_acc)
+    axarr[1].set_title("acc (avg), top: %f" % max(config.epoch_avg_subhead_acc))
+
+    axarr[2].clear()
+    axarr[2].plot(config.epoch_loss_head_A)
+    axarr[2].set_title("Loss head A")
+
+    axarr[3].clear()
+    axarr[3].plot(config.epoch_loss_no_lamb_head_A)
+    axarr[3].set_title("Loss no lamb head A")
+
+    axarr[4].clear()
+    axarr[4].plot(config.epoch_loss_head_B)
+    axarr[4].set_title("Loss head B")
+
+    axarr[5].clear()
+    axarr[5].plot(config.epoch_loss_no_lamb_head_B)
+    axarr[5].set_title("Loss no lamb head B")
+
+    if config.double_eval:
+        axarr[6].clear()
+        axarr[6].plot(config.double_eval_acc)
+        axarr[6].set_title("double eval acc (best), top: %f" %
+                           max(config.double_eval_acc))
+
+        axarr[7].clear()
+        axarr[7].plot(config.double_eval_avg_subhead_acc)
+        axarr[7].set_title("double eval acc (avg), top: %f" %
+                           max(config.double_eval_avg_subhead_acc))
+
+    fig.tight_layout()
+    fig.canvas.draw_idle()
+    fig.savefig(os.path.join(config.out_dir, "plots.png"))
+
+    if is_best or (e_i % config.save_freq == 0):
+        net.module.cpu()
+
+        if e_i % config.save_freq == 0:
+            torch.save(net.module.state_dict(),
+                       os.path.join(config.out_dir, "latest_net.pytorch"))
+            torch.save(optimiser.state_dict(),
+                       os.path.join(config.out_dir, "latest_optimiser.pytorch"))
+            config.last_epoch = e_i  # for last saved version
+
+        if is_best:
+            torch.save(net.module.state_dict(),
+                       os.path.join(config.out_dir, "best_net.pytorch"))
+            torch.save(optimiser.state_dict(),
+                       os.path.join(config.out_dir, "best_optimiser.pytorch"))
+            with open(os.path.join(config.out_dir, "best_config.pickle"),
+                      'wb') as outfile:
+                pickle.dump(config, outfile)
+
+            with open(os.path.join(config.out_dir, "best_config.txt"),
+                      "w") as text_file:
+                text_file.write("%s" % config)
+
+        net.module.cuda()
+
+    with open(os.path.join(config.out_dir, "config.pickle"),
+              'wb') as outfile:
         pickle.dump(config, outfile)
 
-      with open(os.path.join(config.out_dir, "best_config.txt"),
-                "w") as text_file:
+    with open(os.path.join(config.out_dir, "config.txt"),
+              "w") as text_file:
         text_file.write("%s" % config)
 
-    net.module.cuda()
-
-  with open(os.path.join(config.out_dir, "config.pickle"),
-            'wb') as outfile:
-    pickle.dump(config, outfile)
-
-  with open(os.path.join(config.out_dir, "config.txt"),
-            "w") as text_file:
-    text_file.write("%s" % config)
-
-  if config.test_code:
-    exit(0)
+    if config.test_code:
+        exit(0)
diff --git a/code/utils/cluster/YT_BB_data.py b/code/utils/cluster/YT_BB_data.py
new file mode 100644
index 0000000..03ad766
--- /dev/null
+++ b/code/utils/cluster/YT_BB_data.py
@@ -0,0 +1,201 @@
+import sys
+from datetime import datetime
+
+import torch
+import torchvision
+from torch.utils.data import ConcatDataset
+
+from code.datasets.clustering.truncated_dataset import TruncatedDataset
+from code.utils.cluster.transforms import sobel_make_transforms, \
+  greyscale_make_transforms
+from code.utils.semisup.dataset import TenCropAndFinish
+from .general import reorder_train_deterministic
+
+from code.datasets.clustering.YT_BB import YT_BB
+from code.datasets.clustering.YT_BB_5 import YT_BB_5
+
+
+# Used by sobel and greyscale clustering twohead scripts -----------------------
+
+def cluster_twohead_create_YT_BB_dataloaders(config):
+  assert (config.mode == "IID")
+  assert (config.twohead)
+
+  if config.dataset == "YT_BB":
+    config.train_partitions_head_A = config.train_partition
+    config.train_partitions_head_B = config.train_partitions_head_A
+
+    config.mapping_assignment_partitions = config.assignment_partition
+    config.mapping_test_partitions = config.test_partition
+
+    dataset_class = YT_BB  #TODO YT_BB custom class
+
+    # datasets produce either 2 or 5 channel images based on config.include_rgb
+    tf1, tf2, tf3 = sobel_make_transforms(config)
+  else:
+    assert (False)
+
+  print("Making datasets with YT_BB")
+  sys.stdout.flush()
+
+  dataloaders_head_A = \
+    _create_dataloaders(config, dataset_class, tf1, tf2,
+                        partition=config.train_partitions_head_A,
+                       )
+
+  dataloaders_head_B = \
+    _create_dataloaders(config, dataset_class, tf1, tf2,
+                        partition=config.train_partitions_head_B,
+                       )
+
+  mapping_assignment_dataloader = \
+    _create_mapping_loader(config, dataset_class, tf3,
+                           partition=config.mapping_assignment_partitions
+                           )
+
+  mapping_test_dataloader = \
+    _create_mapping_loader(config, dataset_class, tf3,
+                           partition=config.mapping_test_partitions
+                          )
+
+  return dataloaders_head_A, dataloaders_head_B, \
+         mapping_assignment_dataloader, mapping_test_dataloader
+
+
+# Data creation helpers --------------------------------------------------------
+
+def _create_dataloaders(config, dataset_class, tf1, tf2,
+                        partition, shuffle=False):
+  curr_frame = int(config.base_frame)
+  train_imgs_list = []
+  for i in xrange(config.base_num):
+    train_imgs_curr = dataset_class(root=config.dataset_root,
+                                    transform=tf1,
+                                    frame=curr_frame + config.base_interval * i,
+                                    crop=config.crop_by_bb,
+                                    partition=partition)
+    train_imgs_list.append(train_imgs_curr)
+
+  train_imgs = ConcatDataset(train_imgs_list)
+  train_dataloader = torch.utils.data.DataLoader(train_imgs,
+                                                 batch_size=config.dataloader_batch_sz,
+                                                 shuffle=shuffle,
+                                                 num_workers=0,
+                                                 drop_last=False)
+
+  if not shuffle:
+    assert (isinstance(train_dataloader.sampler,
+                       torch.utils.data.sampler.SequentialSampler))
+  dataloaders = [train_dataloader]
+
+  for d_i in xrange(config.num_dataloaders):
+    print 'Include increment: ' + str(config.frame_increment)
+    print 'd_i: ' + str(d_i)
+    if config.frame_increment:
+      curr_frame = curr_frame + config.interval
+    else:
+      if d_i == 0:
+        curr_frame = curr_frame + config.interval
+    print("Creating auxiliary dataloader ind %d out of %d time %s" %
+          (d_i, config.num_dataloaders, datetime.now()))
+    sys.stdout.flush()
+
+    train_tf_imgs_list = []
+    # for each base train dataset, create corresponding transformed dataset,
+    # then concat together.
+    for i in xrange(config.base_num):
+      this_base_frame = config.base_frame + config.base_interval * i
+      this_tf_frame = (curr_frame + config.base_interval * i) % 19
+      if this_tf_frame > 9:
+        this_tf_frame = this_base_frame - (this_tf_frame - 9)
+      train_imgs_tf_curr = dataset_class(root=config.dataset_root,
+                                         transform=tf2,
+	                                 frame=this_tf_frame,
+                                         crop=config.crop_by_bb,
+                                         partition=partition)
+
+      train_tf_imgs_list.append(train_imgs_tf_curr)
+
+    train_imgs_tf = ConcatDataset(train_tf_imgs_list)
+    train_tf_dataloader = \
+      torch.utils.data.DataLoader(train_imgs_tf,
+                                  batch_size=config.dataloader_batch_sz,
+                                  shuffle=shuffle,
+                                  num_workers=0,
+                                  drop_last=False)
+
+    if not shuffle:
+      assert (isinstance(train_tf_dataloader.sampler,
+                         torch.utils.data.sampler.SequentialSampler))
+    # Verify the length for each dataloader is the same.
+    assert (len(train_dataloader) == len(train_tf_dataloader))
+    dataloaders.append(train_tf_dataloader)
+
+  num_train_batches = len(dataloaders[0])
+  print("Length of datasets vector %d" % len(dataloaders))
+  print("Number of batches per epoch: %d" % num_train_batches)
+  sys.stdout.flush()
+
+  return dataloaders
+
+
+def _create_mapping_loader(config, dataset_class, tf3, partition, 
+                           truncate=False, truncate_pc=None,
+                           tencrop=False,
+                           shuffle=False):
+  if truncate:
+    print("Note: creating mapping loader with truncate == True")
+
+  if tencrop:
+    assert (tf3 is None)
+
+  imgs_list = []
+  if config.test_on_all_frame and partition == "test":
+    for i in xrange(10):
+      imgs_curr = dataset_class(root=config.dataset_root,
+                                transform=tf3,
+                                frame=i,
+                                crop=config.crop_by_bb,
+                                partition=partition)
+  
+      if truncate:
+        print("shrinking dataset from %d" % len(imgs_curr))
+        imgs_curr = TruncatedDataset(imgs_curr, pc=truncate_pc)
+        print("... to %d" % len(imgs_curr))
+
+      if tencrop:
+        imgs_curr = TenCropAndFinish(imgs_curr, input_sz=config.input_sz,
+                                 include_rgb=config.include_rgb)
+
+      imgs_list.append(imgs_curr)
+  else:
+    for i in xrange(config.base_num):
+      imgs_curr = dataset_class(root=config.dataset_root,
+                                transform=tf3,
+                                frame=config.base_frame + config.base_interval * i,
+                                crop=config.crop_by_bb,
+                                partition=partition)
+  
+      if truncate:
+        print("shrinking dataset from %d" % len(imgs_curr))
+        imgs_curr = TruncatedDataset(imgs_curr, pc=truncate_pc)
+        print("... to %d" % len(imgs_curr))
+
+      if tencrop:
+        imgs_curr = TenCropAndFinish(imgs_curr, input_sz=config.input_sz,
+                                   include_rgb=config.include_rgb)
+  
+      imgs_list.append(imgs_curr)
+
+  imgs = ConcatDataset(imgs_list)
+  dataloader = torch.utils.data.DataLoader(imgs,
+                                           batch_size=config.batch_sz,
+                                           # full batch
+                                           shuffle=shuffle,
+                                           num_workers=0,
+                                           drop_last=False)
+
+  if not shuffle:
+    assert (isinstance(dataloader.sampler,
+                       torch.utils.data.sampler.SequentialSampler))
+  return dataloader
diff --git a/code/utils/cluster/cluster_eval.py b/code/utils/cluster/cluster_eval.py
index 7ca93d2..ee1e8cf 100644
--- a/code/utils/cluster/cluster_eval.py
+++ b/code/utils/cluster/cluster_eval.py
@@ -113,9 +113,31 @@ def cluster_subheads_eval(config, net,
     best_sub_head = best_sub_head_eval
 
   if config.mode == "IID":
-    assert (
-      config.mapping_assignment_partitions == config.mapping_test_partitions)
-    test_accs = train_accs
+    #assert (
+    #  config.mapping_assignment_partitions == config.mapping_test_partitions)
+    #test_accs = train_accs
+    flat_predss_all, flat_targets_all, = \
+      get_data_fn(config, net, mapping_test_dataloader, sobel=sobel,
+                  using_IR=using_IR,
+                  verbose=verbose)
+
+    num_samples = flat_targets_all.shape[0]
+    test_accs = np.zeros(config.num_sub_heads, dtype=np.float32)
+    for i in xrange(config.num_sub_heads):
+      reordered_preds = torch.zeros(num_samples,
+                                    dtype=flat_predss_all[0].dtype).cuda()
+      for pred_i, target_i in all_matches[i]:
+        reordered_preds[flat_predss_all[i] == pred_i] = target_i
+      test_acc = _acc(reordered_preds, flat_targets_all, config.gt_k, verbose=0)
+
+      test_accs[i] = test_acc
+      test_all_matches, test_accs_2 = _get_assignment_data_matches(net,
+                                                         mapping_test_dataloader,
+                                                         config,
+                                                         sobel=sobel,
+                                                         using_IR=using_IR,
+                                                         get_data_fn=get_data_fn,
+                                                         verbose=verbose)
   elif config.mode == "IID+":
     flat_predss_all, flat_targets_all, = \
       get_data_fn(config, net, mapping_test_dataloader, sobel=sobel,
@@ -137,12 +159,18 @@ def cluster_subheads_eval(config, net,
 
   return {"test_accs": list(test_accs),
           "avg": np.mean(test_accs),
+          "train_avg": np.mean(train_accs),
           "std": np.std(test_accs),
-          "best": test_accs[best_sub_head],
+          "train_std": np.std(train_accs),
           "worst": test_accs.min(),
+          "train_worst": train_accs.min(),
           "best_train_sub_head": best_sub_head,  # from training data
           "best_train_sub_head_match": all_matches[best_sub_head],
-          "train_accs": list(train_accs)}
+          "train_accs": list(train_accs),
+          "test_accs_2": list(test_accs_2),
+          "train_best": train_accs.max(),
+          "best": test_accs[best_sub_head],
+          "test_best_2": test_accs_2.max()}
 
 
 def _get_assignment_data_matches(net, mapping_assignment_dataloader, config,
@@ -353,10 +381,14 @@ def cluster_eval(config, net, mapping_assignment_dataloader,
     print(stats_dict)
   else:
     acc = stats_dict["best"]
+    best_train_acc = stats_dict["train_best"]
+    best_test_acc_2 = stats_dict["test_best_2"]
     is_best = (len(config.epoch_acc) > 0) and (acc > max(config.epoch_acc))
 
     config.epoch_stats.append(stats_dict)
     config.epoch_acc.append(acc)
+    config.epoch_train_acc.append(best_train_acc)
+    config.epoch_test_acc_2.append(best_test_acc_2)
     config.epoch_avg_subhead_acc.append(stats_dict["avg"])
 
-    return is_best
\ No newline at end of file
+    return is_best
diff --git a/code/utils/cluster/transforms.py b/code/utils/cluster/transforms.py
index 7179e70..53157cc 100644
--- a/code/utils/cluster/transforms.py
+++ b/code/utils/cluster/transforms.py
@@ -113,12 +113,22 @@ def sobel_make_transforms(config, random_affine=False,
   tf2_list = []
   tf3_list = []
   if config.crop_orig:
-    tf1_list += [
-      torchvision.transforms.RandomCrop(tuple(np.array([config.rand_crop_sz,
-                                                        config.rand_crop_sz]))),
-      torchvision.transforms.Resize(tuple(np.array([config.input_sz,
-                                                    config.input_sz]))),
-    ]
+    if not config.center_crop_x:
+      print ('random crop x')
+      tf1_list += [
+        torchvision.transforms.RandomCrop(tuple(np.array([config.rand_crop_sz,
+                                                          config.rand_crop_sz]))),
+        torchvision.transforms.Resize(tuple(np.array([config.input_sz,
+                                                      config.input_sz]))),
+      ]
+    else:
+      print ('center crop x')
+      tf1_list += [
+        torchvision.transforms.CenterCrop(tuple(np.array([config.rand_crop_sz,
+                                                          config.rand_crop_sz]))),
+        torchvision.transforms.Resize(tuple(np.array([config.input_sz,
+                                                      config.input_sz]))),
+      ]
     tf3_list += [
       torchvision.transforms.CenterCrop(tuple(np.array([config.rand_crop_sz,
                                                         config.rand_crop_sz]))),
@@ -143,11 +153,16 @@ def sobel_make_transforms(config, random_affine=False,
       print("adding crop size option for imgs_tf: %d" % crop_sz)
       imgs_tf_crops.append(torchvision.transforms.RandomCrop(crop_sz))
     tf2_list += [torchvision.transforms.RandomChoice(imgs_tf_crops)]
-  else:
-    # default
+  elif config.rand_crop_x_prime:
+    print ('include random crop in transformation g')
     tf2_list += [
       torchvision.transforms.RandomCrop(tuple(np.array([config.rand_crop_sz,
                                                         config.rand_crop_sz])))]
+  elif config.center_crop_x_prime:
+    print ('include center crop in transformation g')
+    tf2_list += [
+      torchvision.transforms.CenterCrop(tuple(np.array([config.rand_crop_sz,
+                                                        config.rand_crop_sz])))]
 
   if random_affine:
     print("adding affine with p %f" % affine_p)
@@ -180,14 +195,15 @@ def sobel_make_transforms(config, random_affine=False,
     )
   else:
     print("not using cutout")
-
-  tf2_list += [
-    torchvision.transforms.Resize(tuple(np.array([config.input_sz,
-                                                  config.input_sz]))),
-    torchvision.transforms.RandomHorizontalFlip(),
-    torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4,
-                                       saturation=0.4, hue=0.125)
-  ]
+  if not config.remove_g:
+    print ("add transformation g")
+    tf2_list += [
+      torchvision.transforms.Resize(tuple(np.array([config.input_sz,
+                                                    config.input_sz]))),
+      torchvision.transforms.RandomHorizontalFlip(),
+      torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4,
+                                         saturation=0.4, hue=0.125)
+    ]
 
   tf2_list.append(custom_greyscale_to_tensor(config.include_rgb))
 
diff --git a/datasets/data.py b/datasets/data.py
new file mode 100644
index 0000000..e2a3c11
--- /dev/null
+++ b/datasets/data.py
@@ -0,0 +1,43 @@
+import os
+import torch
+import torchvision.datasets as datasets
+
+__DATASETS_DEFAULT_PATH = "/users/k1763920/IIC/datasets"
+
+def get_dataset(name, split='train', transform=None,
+                target_transform=None, download=True, datasets_path=__DATASETS_DEFAULT_PATH):
+    train = (split == 'train')
+    root = os.path.join(datasets_path, name)
+    if name == 'cifar10':
+        return datasets.CIFAR10(root=root,
+                                train=train,
+                                transform=transform,
+                                target_transform=target_transform,
+                                download=download)
+    elif name == 'cifar100':
+        return datasets.CIFAR100(root=root,
+                                 train=train,
+                                 transform=transform,
+                                 target_transform=target_transform,
+                                 download=download)
+    elif name == 'mnist':
+        return datasets.MNIST(root=root,
+                              train=train,
+                              transform=transform,
+                              target_transform=target_transform,
+                              download=download)
+    elif name == 'stl10':
+        return datasets.STL10(root=root,
+                              split=split,
+                              transform=transform,
+                              target_transform=target_transform,
+                              download=download)
+    elif name == 'imagenet':
+        if train:
+            root = os.path.join(root, 'train')
+        else:
+            root = os.path.join(root, 'val')
+        return datasets.ImageFolder(root=root,
+                                    transform=transform,
+                                    target_transform=target_transform) 
+get_dataset("stl10")
diff --git a/examples/commands.txt b/examples/commands.txt
index 3b0dd8f..62b6fcd 100644
--- a/examples/commands.txt
+++ b/examples/commands.txt
@@ -15,10 +15,10 @@ Note there are two settings that can be used to evaluate models (selecting sub-h
 1.1 Fully unsupervised image clustering - table 1, figures 1, 3, 5
 
   STL10 (569)
-  export CUDA_VISIBLE_DEVICES=0 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 569 --arch ClusterNet5gTwoHead --mode IID --dataset STL10 --dataset_root /scratch/local/ssd/xuji/STL --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --mix_train --crop_orig --rand_crop_sz 64 --input_sz 64 --head_A_first --double_eval --batchnorm_track > out/gnodee2_0_m569.out &
+  export CUDA_VISIBLE_DEVICES=0 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 569 --arch ClusterNet5gTwoHead --mode IID --dataset STL10 --dataset_root /users/k1763920/IIC/datasets/stl10 --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --mix_train --crop_orig --rand_crop_sz 64 --input_sz 64 --head_A_first --double_eval --batchnorm_track > out/gnodee2_0_m569.out &
 
   or (570):
-  export CUDA_VISIBLE_DEVICES=0 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 570  --arch ClusterNet5gTwoHead --mode IID --dataset STL10 --dataset_root /scratch/local/ssd/xuji/STL --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --mix_train --crop_orig --rand_crop_sz 64 --input_sz 64 --head_B_epochs 2 --double_eval --batchnorm_track > out/gnodee3_0_m570.out &
+  export CUDA_VISIBLE_DEVICES=0 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 570  --arch ClusterNet5gTwoHead --mode IID --dataset STL10 --dataset_root /users/k1763920/IIC/datasets/stl10 --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --mix_train --crop_orig --rand_crop_sz 64 --input_sz 64 --head_B_epochs 2 --double_eval --batchnorm_track > out/gnodee3_0_m570.out &
 
   CIFAR10 (640)
   export CUDA_VISIBLE_DEVICES=2 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 640  --arch ClusterNet5gTwoHead --mode IID --dataset CIFAR10 --dataset_root /scratch/local/ssd/xuji/CIFAR --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --crop_orig --rand_crop_sz 20 --input_sz 32 --head_A_first --head_B_epochs 2 > out/gnoded1_gpu2_m640_r1.out &
@@ -114,4 +114,4 @@ Note there are two settings that can be used to evaluate models (selecting sub-h
 
   export CUDA_VISIBLE_DEVICES=0,1 && nohup python -m code.scripts.cluster.analysis.print_sub_heads_eval --model_inds 569 570 640 579 685 > subheads_lowest_loss.out &
 
-  export CUDA_VISIBLE_DEVICES=0,1 && nohup python -m code.scripts.cluster.analysis.print_sub_heads_eval --model_inds 692 693 694 695 569 > subheads_ablation_lowest_loss.out &
\ No newline at end of file
+  export CUDA_VISIBLE_DEVICES=0,1 && nohup python -m code.scripts.cluster.analysis.print_sub_heads_eval --model_inds 692 693 694 695 569 > subheads_ablation_lowest_loss.out &
diff --git a/memory_test.sh b/memory_test.sh
new file mode 100644
index 0000000..49b4f8f
--- /dev/null
+++ b/memory_test.sh
@@ -0,0 +1,7 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+
+cat /proc/meminfo
diff --git a/model_10011.sh b/model_10011.sh
new file mode 100644
index 0000000..f4db381
--- /dev/null
+++ b/model_10011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 10011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --remove_g
diff --git a/model_10012.sh b/model_10012.sh
new file mode 100644
index 0000000..e7fdb8d
--- /dev/null
+++ b/model_10012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 10012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --remove_g
diff --git a/model_10111.sh b/model_10111.sh
new file mode 100644
index 0000000..a569f1f
--- /dev/null
+++ b/model_10111.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 10111 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --remove_g
diff --git a/model_10112.sh b/model_10112.sh
new file mode 100644
index 0000000..1b59a52
--- /dev/null
+++ b/model_10112.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 10112 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --remove_g
diff --git a/model_1100.sh b/model_1100.sh
new file mode 100644
index 0000000..aa0903f
--- /dev/null
+++ b/model_1100.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 1100 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 0 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track --restart
diff --git a/model_1102.sh b/model_1102.sh
new file mode 100644
index 0000000..675ac3c
--- /dev/null
+++ b/model_1102.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 1102 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 100 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_1112.sh b/model_1112.sh
new file mode 100644
index 0000000..6c5a728
--- /dev/null
+++ b/model_1112.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 1112 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 100 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_1212.sh b/model_1212.sh
new file mode 100644
index 0000000..dc55fe4
--- /dev/null
+++ b/model_1212.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 1212 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 100 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_2000.sh b/model_2000.sh
new file mode 100644
index 0000000..6304c64
--- /dev/null
+++ b/model_2000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 0 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_2001.sh b/model_2001.sh
new file mode 100644
index 0000000..ebcfe19
--- /dev/null
+++ b/model_2001.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2001 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 1 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_2002.sh b/model_2002.sh
new file mode 100644
index 0000000..07a9124
--- /dev/null
+++ b/model_2002.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2002 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 2 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_2003.sh b/model_2003.sh
new file mode 100644
index 0000000..39dc4d7
--- /dev/null
+++ b/model_2003.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2003 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 3 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_2004.sh b/model_2004.sh
new file mode 100644
index 0000000..85c96ad
--- /dev/null
+++ b/model_2004.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2004 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 4 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_2005.sh b/model_2005.sh
new file mode 100644
index 0000000..f5a55b7
--- /dev/null
+++ b/model_2005.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2005 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 1 --interval 5 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_211.sh b/model_211.sh
new file mode 100644
index 0000000..9ee1933
--- /dev/null
+++ b/model_211.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 211 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_212.sh b/model_212.sh
new file mode 100644
index 0000000..d552f1b
--- /dev/null
+++ b/model_212.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 212 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 100 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_2212.sh b/model_2212.sh
new file mode 100644
index 0000000..251a87f
--- /dev/null
+++ b/model_2212.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 2212 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 100 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --center_crop_x_prime --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_3000.sh b/model_3000.sh
new file mode 100644
index 0000000..0e8a2af
--- /dev/null
+++ b/model_3000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 0 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3001.sh b/model_3001.sh
new file mode 100644
index 0000000..b9419a2
--- /dev/null
+++ b/model_3001.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3001 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3002.sh b/model_3002.sh
new file mode 100644
index 0000000..7671b1a
--- /dev/null
+++ b/model_3002.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3002 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3003.sh b/model_3003.sh
new file mode 100644
index 0000000..0fdcdba
--- /dev/null
+++ b/model_3003.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3003 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 3 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3004.sh b/model_3004.sh
new file mode 100644
index 0000000..3cf67c9
--- /dev/null
+++ b/model_3004.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3004 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 4 --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3011.sh b/model_3011.sh
new file mode 100644
index 0000000..f5f37fc
--- /dev/null
+++ b/model_3011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_3012.sh b/model_3012.sh
new file mode 100644
index 0000000..da84fb1
--- /dev/null
+++ b/model_3012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 3012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --crop_by_bb --train_partition 'train' --test_partition 'test' --test_on_all_frame
diff --git a/model_311.sh b/model_311.sh
new file mode 100644
index 0000000..12f40a1
--- /dev/null
+++ b/model_311.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 311 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_5000.sh b/model_5000.sh
new file mode 100644
index 0000000..3b8d47b
--- /dev/null
+++ b/model_5000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 5000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 0 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track --restart
diff --git a/model_50000.sh b/model_50000.sh
new file mode 100644
index 0000000..b711624
--- /dev/null
+++ b/model_50000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 50000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_50002.sh b/model_50002.sh
new file mode 100644
index 0000000..96d6437
--- /dev/null
+++ b/model_50002.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 50002 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_5001.sh b/model_5001.sh
new file mode 100644
index 0000000..85c98a0
--- /dev/null
+++ b/model_5001.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 5001 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_50012.sh b/model_50012.sh
new file mode 100644
index 0000000..f2cbf02
--- /dev/null
+++ b/model_50012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 50012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_5003.sh b/model_5003.sh
new file mode 100644
index 0000000..cbcf67b
--- /dev/null
+++ b/model_5003.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 5003 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 3 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_5004.sh b/model_5004.sh
new file mode 100644
index 0000000..92dc7d9
--- /dev/null
+++ b/model_5004.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 5004 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 4 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_6000.sh b/model_6000.sh
new file mode 100644
index 0000000..36ac1ad
--- /dev/null
+++ b/model_6000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 6000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 0 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_60011.sh b/model_60011.sh
new file mode 100644
index 0000000..f2e640e
--- /dev/null
+++ b/model_60011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 60011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_60012.sh b/model_60012.sh
new file mode 100644
index 0000000..568fb8f
--- /dev/null
+++ b/model_60012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 60012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_601.sh b/model_601.sh
new file mode 100644
index 0000000..63db3b4
--- /dev/null
+++ b/model_601.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 601 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --crop_by_bb --frame_increment --train_partition 'train' --test_partition 'test'
diff --git a/model_6011.sh b/model_6011.sh
new file mode 100644
index 0000000..0d3516f
--- /dev/null
+++ b/model_6011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 6011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_6012.sh b/model_6012.sh
new file mode 100644
index 0000000..af515ab
--- /dev/null
+++ b/model_6012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 6012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame --batchnorm_track
diff --git a/model_602.sh b/model_602.sh
new file mode 100644
index 0000000..d56cc28
--- /dev/null
+++ b/model_602.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 602 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --crop_by_bb --frame_increment --train_partition 'train' --test_partition 'test'
diff --git a/model_603.sh b/model_603.sh
new file mode 100644
index 0000000..45079f5
--- /dev/null
+++ b/model_603.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 603 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 3 --crop_by_bb --frame_increment --train_partition 'train' --test_partition 'test'
diff --git a/model_7003.sh b/model_7003.sh
new file mode 100644
index 0000000..15b2288
--- /dev/null
+++ b/model_7003.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 7003 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 3 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_701.sh b/model_701.sh
new file mode 100644
index 0000000..3536333
--- /dev/null
+++ b/model_701.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 701 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --crop_by_bb --frame_increment --train_partition 'train' --test_partition 'test'
diff --git a/model_801.sh b/model_801.sh
new file mode 100644
index 0000000..da2f4ce
--- /dev/null
+++ b/model_801.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 801 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 10 --num_sub_heads 5 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --crop_by_bb --frame_increment --train_partition 'train' --test_partition 'test'
diff --git a/model_8011.sh b/model_8011.sh
new file mode 100644
index 0000000..5a14424
--- /dev/null
+++ b/model_8011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 8011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_8012.sh b/model_8012.sh
new file mode 100644
index 0000000..b129db3
--- /dev/null
+++ b/model_8012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 8012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --rand_crop --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9000.sh b/model_9000.sh
new file mode 100644
index 0000000..bb5faa8
--- /dev/null
+++ b/model_9000.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9000 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 0 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9001.sh b/model_9001.sh
new file mode 100644
index 0000000..33e3cde
--- /dev/null
+++ b/model_9001.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9001 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9002.sh b/model_9002.sh
new file mode 100644
index 0000000..68f9b49
--- /dev/null
+++ b/model_9002.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9002 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9003.sh b/model_9003.sh
new file mode 100644
index 0000000..3bc8be2
--- /dev/null
+++ b/model_9003.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9003 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 3 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9004.sh b/model_9004.sh
new file mode 100644
index 0000000..e05dd98
--- /dev/null
+++ b/model_9004.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9004 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 4 --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9011.sh b/model_9011.sh
new file mode 100644
index 0000000..33f8dbf
--- /dev/null
+++ b/model_9011.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9011 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 1 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/model_9012.sh b/model_9012.sh
new file mode 100644
index 0000000..68df460
--- /dev/null
+++ b/model_9012.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.YT_BB_script --model_ind 9012 --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb_small" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --input_sz 32 --crop_orig --rand_crop_sz 20 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 1 --base_num 10 --interval 2 --frame_increment --train_partition 'train' --test_partition 'test' --assignment_partition 'train' --test_on_all_frame
diff --git a/presentation/presentation1.ppt b/presentation/presentation1.ppt
new file mode 100644
index 0000000..211f692
Binary files /dev/null and b/presentation/presentation1.ppt differ
diff --git a/presentation/presentation2.ppt b/presentation/presentation2.ppt
new file mode 100644
index 0000000..e96e3f0
Binary files /dev/null and b/presentation/presentation2.ppt differ
diff --git a/presentation/presentation3.pdf b/presentation/presentation3.pdf
new file mode 100644
index 0000000..75f72db
Binary files /dev/null and b/presentation/presentation3.pdf differ
diff --git a/presentation/presentation3.ppt b/presentation/presentation3.ppt
new file mode 100644
index 0000000..953555e
Binary files /dev/null and b/presentation/presentation3.ppt differ
diff --git a/presentation/presentation4.pdf b/presentation/presentation4.pdf
new file mode 100644
index 0000000..46fa9d3
Binary files /dev/null and b/presentation/presentation4.pdf differ
diff --git a/presentation/presentation4.ppt b/presentation/presentation4.ppt
new file mode 100644
index 0000000..eda11bb
Binary files /dev/null and b/presentation/presentation4.ppt differ
diff --git a/presentation/presentation5.pdf b/presentation/presentation5.pdf
new file mode 100644
index 0000000..635509d
Binary files /dev/null and b/presentation/presentation5.pdf differ
diff --git a/presentation/presentation5.ppt b/presentation/presentation5.ppt
new file mode 100644
index 0000000..6ed5223
Binary files /dev/null and b/presentation/presentation5.ppt differ
diff --git a/pytorch_build.sh b/pytorch_build.sh
new file mode 100644
index 0000000..f725bb6
--- /dev/null
+++ b/pytorch_build.sh
@@ -0,0 +1,11 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --mem=30000
+"#SBATCH --constrain=v100"
+
+conda activate myenv
+export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
+python setup.py install
+
diff --git a/stl10_test_2.sh b/stl10_test_2.sh
new file mode 100644
index 0000000..ede2537
--- /dev/null
+++ b/stl10_test_2.sh
@@ -0,0 +1,11 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 569 --arch ClusterNet5gTwoHead --mode IID --dataset STL10 --dataset_root /users/k1763920/IIC/datasets/stl10 --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.001  --num_epochs 2000 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --mix_train --crop_orig --rand_crop_sz 64 --input_sz 64 --head_A_first --double_eval --batchnorm_track --stl_leave_out_unlabelled
diff --git a/test_trained_model.py b/test_trained_model.py
new file mode 100644
index 0000000..c7df5e7
--- /dev/null
+++ b/test_trained_model.py
@@ -0,0 +1,9 @@
+import pickle
+import torch
+import code.archs as archs
+
+config_in = open("config.pickle", "rb")
+config = pickle.load(config_in)
+net = archs.__dict__[config.arch](config)
+net.load_state_dict(torch.load("best_net.pytorch"))
+net.eval()
diff --git a/yt_bb.sh b/yt_bb.sh
new file mode 100644
index 0000000..62ce645
--- /dev/null
+++ b/yt_bb.sh
@@ -0,0 +1,12 @@
+#!/bin/bash -l
+#SBATCH --output=/mnt/lustre/users/%u/%j.out
+#SBATCH --mem=30000
+#SBATCH --job-name=gpu
+#SBATCH --gres=gpu
+#SBATCH --constrain=v100
+#SBATCH --time=4-0:00
+
+conda activate py2
+#module load libs/cuda
+
+export CUDA_VISIBLE_DEVICES=0 && nohup python -m code.scripts.cluster.YT_BB_script --model_ind 3  --arch ClusterNet5gTwoHead --mode IID --dataset YT_BB --dataset_root "/users/k1763920/yt_bb" --out_root "/users/k1763920/out/" --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 5 --num_sub_heads 5 --crop_orig --rand_crop_sz 20 --input_sz 32 --head_A_first --head_B_epochs 2 --base_frame 0 --base_interval 2 --base_num 5 --interval 3 --crop_by_bb > "/users/k1763920/out/3.out" &
